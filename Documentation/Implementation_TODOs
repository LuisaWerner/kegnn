TODO



CLEAN THE GIT
- merge relevant and stable code into the master
- delete unnecessary files                                                                                              [Luisa, started 12.08.]


KENN LAYER
- find alternative to scatter ? (faster)
- allessandros code (with batching?) → make sure it’s slower than scatter
    - ask alessandro why he doesn’t use scatter: https://github.com/HEmile/KENN-PyTorch/issues/4

- accelerate loop of clause enhancers in KnowledgeEnhancer.py


DDP/Performance
- try with 2 machines
- parallelize validation and test
- try PytochLightning
- Evaluation every x epochs                                                                                             [Luisa, started 12.08.]
- determine when KENN crashes (on ogb products) because of OOM
- M1 vs server

experiments
- understand performance of  KENN
- KENN_MLP vs GCN
- KENN_GCN vs GCN
- MLP vs GCN
- compare KENN to MLP on arxiv
- prepare conf file
- hyperparameter optimization

paper
- define overall structure
- comparison KENN /GNN
- showing neighborhood explosion theoretically
- time and space complexity of KENN
- neural-symbolic part

Other
- compare current batching methods
- statistics on datasets
- Hyperparameter tuning
    - https://docs.wandb.ai/guides/sweeps
    - define set of hyperparameters that are important for the experiments
    - define the set of knowledge to test (statistics of consistency of knowledge in the data)
- Batching
- error of cluster sampling on arxiv.
    - many batches and small data, works with a low number of partitions
- Adapt ReadMe

RUNS

1. 12.08.22: Goal: Compare different models MLP, GCN, KENN_MLP, KENN_GCN
-full-batch, transductive, ogbn-arxiv


