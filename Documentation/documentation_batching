

BATCHING ALGORITHMS IN KENN IMPLEMENTATION

(1) DEFAULT BATCHING

   Principle Idea
   NeighborLoader takes in this list of num_neighbors and iteratively samples num_neighbors[i] for each node involved in iteration i - 1. The NeighborLoader will return subgraphs where global node indices are mapped to local indices corresponding to this specific subgraph.

    ------------------------------------------------------------------------------------------------------
    Algorithm (based on GraphSAGE)
    K = num_layers_sampling
    b = batch_size
    n = sampling_neighbor_size

    1. Split training set V_train in slices of |batch_size| (last batch < |batch_size|), to obtanin |B| batches, the target nodes of a batch are V_target
    2. For i = 1, ... |B|: 
        For j= 1 ... K: 
            for node v in v_target :
                randomly sample n k-order neighbors 
                append them to |B|
                remove duplicates from |B|
    3. Return list(B_1, B_2, ... |B|)
    ------------------------------------------------------------------------------------------------------
    

    Structure of Batch
    - no duplicated nodes 
    - each target nodes is only in one batch 
    - a node can be neighbor in several batches
    - Sampled nodes are sorted based on the order in which they were sampled.

    ----------------
    | target       |
    | nodes        |
    |              |
    ----------------
    |  1st order   |
    |  neigbor     |
    |  nodes       |
    |   ...        |
    ----------------
    |  n-order     |
    |  neigbor     |
    |  nodes       |
    |              |
    ----------------


    Explanation of IMPLEMENTATION
     - NeighborLoader 
     - Layer-wise sampling: samples args.sampling_neighbor_size nodes per layer 
     - batch_size defines the number of target nodes
     - the target nodes are always the first |batch_size| nodes in the batch
     - Each node appears only in one batch (no duplicates)
    
    Hyperparameters

    - args.batch_size
        The number of target nodes per batch, for which we make the predictions

    - args.num_workers
        Number of workers for parallelization 

    - args.num_layers_sampling
        How many layers (max{#KENN layers, #GNN layers}) does the network have? 
        This is needed to differentiate how many n-order neighbors should be sampled 

    - args.sampling_neighbor_size 
        how many neighbors to sample per layer
        eg. sampling_neighbor_size = 30, num_layers = 2
        sample: [30] * 2 neighbors 

    How to enable
    - is applied if args.train_sampling is set to None
    - In order to make this solvable, it is important to choose batch_size, sampling_neighbor_size and num_layers_sampling consciously

    Memory Complexity of a Batch:
    |batch_size| * feature size + K * batch_size * (|num_neighbors| * feature size)
    can be better if a node has < |num_neighbors| neighbors 

    Corresponding Paper 
    https://arxiv.org/pdf/1706.02216.pdf 

    Corresponding Documentation 
    https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/loader/neighbor_loader.html#NeighborLoader 
    
    Remaining Todos 



(2) Graph SAINT Batching

    Principle Idea: 
    Samples nodes and constructs subgraphs that can be processed as mini-batches. This is done before the training loop starts. In the preprocessing, the probability of sampling a node and an edge is estimated and used to normalize the mini-batch loss. 


    Algorithm 
    ------------------------------------------------------------------------------------------------------
    h: random walk length 
    V_train: training graph 
    n: node budget 
    number of roots: r 

    V_target: r target nodes sampled uniformly at random (with replacement) from V_train
    for v in V_target do: 
        u <-- v
        for d=1 to h do
            u <-- node sampled uniformly at random from u's neighbor
            Vs <-- Vs and {u}
        end for
    end for
    Keep track of C_u,v and C_v
    
    RETURN G_s: node induced subgraph 
    ------------------------------------------------------------------------------------------------------

    Normalization coefficients
    Before training, sampler iteratively samples batches. In the meanwhile, C_v counts how often a node v appears in a batch and C_u,v how often an edge appears in a batch. 
    alpha_u,v is set to C_u,v / C_v and lambda to C_v / N 
    In training, multiply the batch loss by lambda
    
    Hyperparameters to set
    - args.sample_coverage: how many nodes to use for calculating the normalization coefficients
    (if sample_coverage is 0, no normalization is conducted )
    - walk_length: length of random walk (how many neighbors to sample)


    How to enable
    args.train_sampling = 'graph_saint'

    Corresponding Paper
    https://arxiv.org/abs/1907.04931 

    Corresponding Documentation 
    https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/loader/graph_saint.html#GraphSAINTSampler 

    Remaining todos 

(3) ClusterGCN Batching

    Principle Idea
    Before training, form clusters that are informative with ClusterData (based on Metis). To be not to biased, group several partitions together to form a batch. Partitions have approximately equal size 

    Algorithm 
    Input: Graph A, feature X, label Y
    Output: Prediction 
    1. Partition graph into c clusters V1, V2, ... Vc by Metis
    for iter 1, ... max_iter do:
        Randomly choose q clusters t1, ... tq from V without replacement
        Form the subgraph G' with nodes V' = [Vt1, Vt2, ... Vtq]
        Compute loss on the subgraph 
        Conduct Adam update using gradient estimator g 
    Output: representation/prediction 

    Explanation of IMPLEMENTATION

    Hyperparameters to set
    - args.cluster_sampling_partition: how to set the partitions. Set them in a way that partitions are smaller than batch_size so that several partitions can form a batch 

    How to enable

    Corresponding Paper

    Corresponding Documentation 
        My questions: https://github.com/pyg-team/pytorch_geometric/discussions/5030
        

    Remaining todos 